{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ccc11e5",
   "metadata": {},
   "source": [
    "# Merging SVGP ensembles using equivalent observations\n",
    "\n",
    "This notebook illustrates how to use the equivalent observation framework to train an ensemble of GP classification models and to make predictions with it.\n",
    "\n",
    "First, let's load some required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d707111",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gpflow\n",
    "import guepard\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.io as sio\n",
    "\n",
    "from gpflow.utilities import print_summary\n",
    "\n",
    "# The lines below are specific to the notebook format\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>div.output_scroll { height: 150em; }</style>\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6b176f",
   "metadata": {},
   "source": [
    "We now load the banana dataset: This is a binary classification problem with two classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ef0602",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "data = sio.loadmat('../../data/banana.mat')\n",
    "Y = data['banana_Y']\n",
    "X = data['banana_X']\n",
    "\n",
    "x1_lim = [-3.5, 3.5]\n",
    "x2_lim = [-3.5, 3.5]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.axhline(0, color='k', linestyle=\"dashed\", alpha=0.5, linewidth=.5)\n",
    "ax.axvline(0, color='k', linestyle=\"dashed\", alpha=0.5, linewidth=.5)\n",
    "ax.plot(X[Y[:, 0] == 0, 0], X[Y[:, 0] == 0, 1], 'o', color=\"C0\", ms=3, alpha=0.3, label=\"$y=0$\")\n",
    "ax.plot(X[Y[:, 0] == 1, 0], X[Y[:, 0] == 1, 1], 'o', color=\"C1\", ms=3, alpha=0.3, label=\"$y=1$\")\n",
    "\n",
    "ax.set_xlim(x1_lim)\n",
    "ax.set_ylim(x2_lim)\n",
    "\n",
    "ax.axes.xaxis.set_ticklabels([])\n",
    "ax.axes.yaxis.set_ticklabels([])\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"plots/banana_data.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f89180a",
   "metadata": {},
   "source": [
    "We then split the dataset in four, with one subset per quadrant of the input space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07933ce",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Compute masks for the four quadrants\n",
    "maskNE = np.logical_and(X[:, 0] < 0, X[:, 1] >= 0) \n",
    "maskNW = np.logical_and(X[:, 0] >= 0, X[:, 1] >= 0) \n",
    "maskSE = np.logical_and(X[:, 0] < 0, X[:, 1] < 0) \n",
    "maskSW = np.logical_and(X[:, 0] >= 0, X[:, 1] < 0) \n",
    "masks = [maskNE, maskNW, maskSE, maskSW]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(6, 6))\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        k = 2 * i + j\n",
    "        \n",
    "        axes[i,j].axhline(0, color='k', linestyle=\"dashed\", alpha=0.5, linewidth=.5)\n",
    "        axes[i,j].axvline(0, color='k', linestyle=\"dashed\", alpha=0.5, linewidth=.5)\n",
    "\n",
    "        X_ = X[masks[k], :]\n",
    "        Y_ = Y[masks[k], :]\n",
    "        axes[i, j].plot(X_[Y_[:, 0] == 0, 0], X_[Y_[:, 0] == 0, 1], 'o', color=\"C0\", ms=3, alpha=0.13, label=\"$y=0$\")\n",
    "        axes[i, j].plot(X_[Y_[:, 0] == 1, 0], X_[Y_[:, 0] == 1, 1], 'o', color=\"C1\", ms=3, alpha=0.13, label=\"$y=1$\")\n",
    "\n",
    "        axes[i, j].plot(X[Y[:, 0] == 0, 0], X[Y[:, 0] == 0, 1], 'o', color=\"C0\", ms=3, alpha=0.02, label=\"$y=0$\")\n",
    "        axes[i, j].plot(X[Y[:, 0] == 1, 0], X[Y[:, 0] == 1, 1], 'o', color=\"C1\", ms=3, alpha=0.02, label=\"$y=1$\")\n",
    "\n",
    "        axes[i, j].set_xticks(np.arange(-3, 4))\n",
    "        axes[i, j].set_yticks(np.arange(-3, 4))\n",
    "        axes[i, j].axes.xaxis.set_ticklabels([])\n",
    "        axes[i, j].axes.yaxis.set_ticklabels([])\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"plots/banana_subdata.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f694e08a",
   "metadata": {},
   "source": [
    "We build an SVGP model for each data subset, with 15 inducing variables for each of them. Note that all submodels share the same kernel and that the kernel parameters are fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d14e5df",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "kernel = gpflow.kernels.Matern32(variance=50., lengthscales=[3., 3.])\n",
    "gpflow.set_trainable(kernel, False)\n",
    "lik = gpflow.likelihoods.Bernoulli()\n",
    "mean_function = gpflow.mean_functions.Zero()\n",
    "\n",
    "M = []\n",
    "for mask in masks:\n",
    "    X_ = X[mask, :]\n",
    "    Y_ = Y[mask, :]\n",
    "    Z = scipy.cluster.vq.kmeans(X_, 15)[0] # the locations of the inducing variables are initialised with k-means\n",
    "\n",
    "    m = gpflow.models.SVGP(inducing_variable=Z, likelihood=lik, kernel=kernel, mean_function=mean_function)\n",
    "    opt = gpflow.optimizers.Scipy()\n",
    "    opt_logs = opt.minimize(m.training_loss_closure((X_, Y_)), m.trainable_variables);\n",
    "    M += [m]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0500ff",
   "metadata": {},
   "source": [
    "Let's plot the submodels predictions in the data space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4177b3",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "x1_grid = np.linspace(*x1_lim, 50)\n",
    "x2_grid = np.linspace(*x2_lim, 50)\n",
    "X1_grid, X2_grid = np.meshgrid(x1_grid, x2_grid) \n",
    "Xtest = np.hstack([X1_grid.reshape(-1, 1), X2_grid.reshape(-1, 1)])\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(6, 6))\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        k = 2 * i + j\n",
    "        \n",
    "        axes[i,j].axhline(0, color='k', linestyle=\"dashed\", alpha=0.2, linewidth=.5)\n",
    "        axes[i,j].axvline(0, color='k', linestyle=\"dashed\", alpha=0.2, linewidth=.5)\n",
    "\n",
    "        X_ = X[masks[k], :]\n",
    "        Y_ = Y[masks[k], :]\n",
    "        axes[i, j].plot(X_[Y_[:, 0] == 0, 0], X_[Y_[:, 0] == 0, 1], 'o', color=\"C0\", ms=3, alpha=0.05, label=\"$y=0$\")\n",
    "        axes[i, j].plot(X_[Y_[:, 0] == 1, 0], X_[Y_[:, 0] == 1, 1], 'o', color=\"C1\", ms=3, alpha=0.05, label=\"$y=1$\")\n",
    "        \n",
    "        Z = M[k].inducing_variable.Z\n",
    "        axes[i, j].plot(Z[:, 0], Z[:, 1], \"ko\", ms=2., alpha=.4)\n",
    "\n",
    "        Ytest, _ = M[k].predict_y(Xtest)\n",
    "        cs = axes[i, j].contour(X1_grid, X2_grid, np.reshape(Ytest, (50, 50)), linewidths=1, colors=[\"C0\", \"C0\", \"grey\",\"C1\", \"C1\"],\n",
    "                        levels=[0.05, 0.25, 0.5, 0.75, 0.95], zorder=0)\n",
    "        axes[i, j].clabel(cs, inline=1, fontsize=10, fmt='%1.2f')\n",
    "\n",
    "\n",
    "        axes[i, j].set_xticks(np.arange(-3, 4))\n",
    "        axes[i, j].set_yticks(np.arange(-3, 4))\n",
    "        axes[i, j].axes.xaxis.set_ticklabels([])\n",
    "        axes[i, j].axes.yaxis.set_ticklabels([])\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"plots/banana_submodels.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b8dd26",
   "metadata": {},
   "source": [
    "We can also plot the submodel predictions in the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919523ff",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def plot_latent(Mtest, Vtest_full, X1_grid, X2_grid, ax, num_sample=100):\n",
    "    Vtest_full = Vtest_full.numpy()[0, :, :]\n",
    "    Vtest = np.diag(Vtest_full).reshape((50, 50))\n",
    "\n",
    "    # plot samples\n",
    "    if num_sample != 0:\n",
    "        Fp = np.random.multivariate_normal(Mtest.numpy().flatten(), Vtest_full, num_sample).T\n",
    "        for k in range(num_sample):\n",
    "            ax.contour(X1_grid, X2_grid, np.reshape(Fp[:, k], (50, 50)),\n",
    "                       levels=np.arange(-4, 4)*2, alpha=0.1,\n",
    "                       linewidths= .5)\n",
    "\n",
    "    # plot mean\n",
    "    cs = ax.contour(X1_grid, X2_grid, np.reshape(Mtest, (50, 50)),\n",
    "                           levels=np.arange(-4, 4)*2,\n",
    "                           linewidths= 1)\n",
    "    ax.clabel(cs, inline=1, fontsize=10, fmt='%1.f')\n",
    "\n",
    "    # add transparency where predictions are uncertain\n",
    "    alphas = np.minimum(1., Vtest/np.max(Vtest))\n",
    "    ax.imshow(np.ones((50, 50)), cmap=\"binary\", alpha=alphas, zorder=2, extent=(*x1_lim, *x2_lim), interpolation=\"bilinear\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(6, 6))\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        k = 2 * i + j\n",
    "        \n",
    "        Z = M[k].inducing_variable.Z\n",
    "        axes[i, j].plot(Z[:, 0], Z[:, 1], \"ko\", ms=2., alpha=.4)\n",
    "        \n",
    "        Ftest, Vtest_full = M[k].predict_f(Xtest, full_cov=True)\n",
    "        plot_latent(Ftest, Vtest_full, X1_grid, X2_grid, axes[i,j], num_sample=50) # The figure in paper has num_sample=50\n",
    "\n",
    "        axes[i,j].axhline(0, color='k', linestyle=\"dashed\", alpha=0.2, linewidth=.5)\n",
    "        axes[i,j].axvline(0, color='k', linestyle=\"dashed\", alpha=0.2, linewidth=.5)\n",
    "\n",
    "        axes[i, j].set_xticks(np.arange(-3, 4))\n",
    "        axes[i, j].set_yticks(np.arange(-3, 4))\n",
    "        axes[i, j].axes.xaxis.set_ticklabels([])\n",
    "        axes[i, j].axes.yaxis.set_ticklabels([])\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"plots/banana_sublatents.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab9e589",
   "metadata": {},
   "source": [
    "We can now use the equivalent observation framework to merge these four submodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e949ef",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "m_agg = guepard.EquivalentObsEnsemble(M)\n",
    "Ftest, Vtest_full = m_agg.predict_f(Xtest, full_cov=True)\n",
    "Ytest = m_agg.predict_y(Xtest)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28adb3df",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "[plt.plot(m.inducing_variable.Z[:, 0], m.inducing_variable.Z[:, 1], \"ko\", ms=2., alpha=.4) for m in m_agg.models]\n",
    "\n",
    "plot_latent(Ftest, Vtest_full, X1_grid, X2_grid, ax, num_sample=100)  # the figure in the paper uses num_sample=100 \n",
    "\n",
    "ax.axhline(0, color='k', linestyle=\"dashed\", alpha=0.5, linewidth=.5)\n",
    "ax.axvline(0, color='k', linestyle=\"dashed\", alpha=0.5, linewidth=.5)\n",
    "\n",
    "ax.set_xlim(x1_lim)\n",
    "ax.set_ylim(x2_lim)\n",
    "ax.axes.xaxis.set_ticklabels([])\n",
    "ax.axes.yaxis.set_ticklabels([])\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"plots/banana_latents.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8dfc76",
   "metadata": {},
   "source": [
    "For comparison we fit an SVGP model with the same kernel, same inducing location Z, but an optimised distribution for the inducing variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e524a3",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "Z = np.vstack([m.inducing_variable.Z for m in m_agg.models])\n",
    "q_mu, q_sigma = m_agg.predict_f(Z, full_cov=True)\n",
    "q_sqrt = np.linalg.cholesky(q_sigma)\n",
    "\n",
    "m_svgp = gpflow.models.SVGP(inducing_variable=Z, likelihood=lik, kernel=kernel, mean_function=mean_function,\n",
    "                      q_mu=q_mu, q_sqrt=q_sqrt, whiten=False)\n",
    "gpflow.set_trainable(m_svgp.inducing_variable, False)\n",
    "\n",
    "opt = gpflow.optimizers.Scipy()\n",
    "opt_logs = opt.minimize(m_svgp.training_loss_closure((X, Y)), m_svgp.trainable_variables);\n",
    "\n",
    "gpflow.set_trainable(m_svgp.inducing_variable, False)\n",
    "\n",
    "opt = gpflow.optimizers.Scipy()\n",
    "opt_logs = opt.minimize(m_svgp.training_loss_closure((X, Y)), m_svgp.trainable_variables)\n",
    "\n",
    "Ysvgp = m_svgp.predict_y(Xtest)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e794f892",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "ax.axhline(0, color='k', linestyle=\"dashed\", alpha=0.5, linewidth=.5)\n",
    "ax.axvline(0, color='k', linestyle=\"dashed\", alpha=0.5, linewidth=.5)\n",
    "\n",
    "ax.plot(X[Y[:, 0] == 0, 0], X[Y[:, 0] == 0, 1], 'o', color=\"C0\", ms=3, alpha=0.05, label=\"$y=0$\")\n",
    "ax.plot(X[Y[:, 0] == 1, 0], X[Y[:, 0] == 1, 1], 'o', color=\"C1\", ms=3, alpha=0.05, label=\"$y=1$\")\n",
    "\n",
    "cs = ax.contour(X1_grid, X2_grid, np.reshape(Ysvgp, (50, 50)), linewidths=1, colors=[\"C0\", \"C0\", \"grey\",\"C1\", \"C1\"],\n",
    "                        levels=[0.05, 0.25, 0.5, 0.75, 0.95], zorder=0, alpha=.5)\n",
    "\n",
    "cs = ax.contour(X1_grid, X2_grid, np.reshape(Ytest, (50, 50)), linewidths=1, colors=[\"C0\", \"C0\", \"grey\",\"C1\", \"C1\"],\n",
    "                        levels=[0.05, 0.25, 0.5, 0.75, 0.95], zorder=0)\n",
    "\n",
    "ax.clabel(cs, inline=1, fontsize=10, fmt='%1.2f')\n",
    "\n",
    "plt.plot(m_svgp.inducing_variable.Z[:, 0], m_svgp.inducing_variable.Z[:, 1], \"ko\", ms=2., alpha=.4)\n",
    "ax.set_xlabel(\"$x_1$\", fontsize=14)\n",
    "ax.set_ylabel(\"$x_2$\", fontsize=14)\n",
    "ax.set_xlim(x1_lim)\n",
    "ax.set_ylim(x2_lim)\n",
    "ax.axes.xaxis.set_ticks([-3, 0, 3])\n",
    "ax.axes.yaxis.set_ticks([-3, 0, 3])\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"plots/banana_models.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bc5125",
   "metadata": {},
   "source": [
    "we can plot the absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45361b8b",
   "metadata": {
    "lines_to_next_cell": 0,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "error = (Ytest- Ysvgp).numpy().flatten()\n",
    "print(\"max absolute error\", np.max(np.abs(error)))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "ax.plot(X[Y[:, 0] == 0, 0], X[Y[:, 0] == 0, 1], 'o', color=\"C0\", ms=3, alpha=0.05, label=\"$y=0$\")\n",
    "ax.plot(X[Y[:, 0] == 1, 0], X[Y[:, 0] == 1, 1], 'o', color=\"C1\", ms=3, alpha=0.05, label=\"$y=1$\")\n",
    "\n",
    "cs = ax.contour(X1_grid, X2_grid, np.reshape(np.abs(error), (50, 50)), linewidths=1, levels=[0.01, 0.02, 0.03, 0.04, 0.05])\n",
    "ax.clabel(cs, inline=1, fontsize=10, fmt='%1.2f')\n",
    "\n",
    "ax.set_xlabel(\"$x_1$\", fontsize=14)\n",
    "ax.set_ylabel(\"$x_2$\", fontsize=14)\n",
    "ax.set_xlim(x1_lim)\n",
    "ax.set_ylim(x2_lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fa0699",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
